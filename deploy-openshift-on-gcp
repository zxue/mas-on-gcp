#
# Deploy IBM Maximo Application Suite using NFS Backed Storage on Google Cloud

Benjamin Xue, Principal Solution Architect at IBM

Jenny Wang, Software Architect at IBM

# Table of Contents

**[Prerequisites 2](#_Toc118371178)**

[GCP Project, Service Account and Role Assignments 2](#_Toc118371179)

[Register a custom domain 3](#_Toc118371180)

[Create a DNS zone in GCP 3](#_Toc118371181)

[Add DNS Name Servers in Domain Registration Account 4](#_Toc118371182)

**[Install Red Hat OpenShift 5](#_Toc118371183)**

[Install OpenShift with the installer 5](#_Toc118371184)

[Install OpenShift with Daffy 6](#_Toc118371185)

**[Use NFS Server backed Storage for OpenShift 8](#_Toc118371186)**

[Create a Filestore Instance 8](#_Toc118371187)

[Deploy a Dynamic Provisioning Storage Class 9](#_Toc118371188)

**[The Infrastructure of OpenShift Deployment on GCP 11](#_Toc118371189)**

[Network Topology 12](#_Toc118371190)

[Cloud Services in OpenShift Deployment 12](#_Toc118371191)

[Infrastructure Architecture 12](#_Toc118371192)

**[Install Maximo Application Suite 13](#_Toc118371193)**

[Define Environment Variables 13](#_Toc118371194)

[Install MAS Core 15](#_Toc118371195)

**[Activate MAS Manage 16](#_Toc118371196)**

[Deploy IBM Cloud Pak for Data 16](#_Toc118371197)

[Create DB2 Warehouse Instance 17](#_Toc118371198)

[Create Database Schema for Manage 17](#_Toc118371199)

[Configure Database Connection 17](#_Toc118371200)

[Activate MAS Manage 18](#_Toc118371201)

[Launch MAS Manage 19](#_Toc118371202)

**[Storages for OpenShift and Maximo 20](#_Toc118371203)**

[OpenShift Storage Classes 20](#_Toc118371204)

[Maximo DB2WH Storage 21](#_Toc118371205)

**[Troubleshoot MAS Manage Activation 22](#_Toc118371206)**

[Check the database connection string in Pods 22](#_Toc118371207)

[Check "ManageWorkspace" custom resource 22](#_Toc118371208)

[Check "jdbc" custom resource 23](#_Toc118371209)

**[Shut Down the OpenShift Cluster 24](#_Toc118371210)**

**[Delete OpenShift Cluster and MAS Deployment 24](#_Toc118371211)**

**[Next Steps 25](#_Toc118371212)**

This document provides detailed instructions on how to deploy Red Hat OpenShift, NFS backed storage and Maximo Application Suite Google Cloud and some troubleshooting tips.

# Prerequisites

Before installing Red Hat OpenShift, you must complete and meet the prerequisites.

## GCP Project, Service Account and Role Assignments

Create a project in your Google Cloud account and a service account with within the project. Grant proper permissions to the service account and enable a set of Google Cloud API services.

The following APIs must be enabled in your GCP project. You can enable the API service using gcloud commands or from Google cloud console.

- iam.googleapis.com
- deploymentmanager.googleapis.com
- compute.googleapis.com
- cloudapis.googleapis.com
- cloudresourcemanager.googleapis.com
- dns.googleapis.com
- iamcredentials.googleapis.com
- servicemanagement.googleapis.com
- serviceusage.googleapis.com
- storage-api.googleapis.com
- storage-component.googleapis.com
- networksecurity.googleapis.com

The following roles must be assigned to the service account based on OpenShift recommended security practices. If the service account is assigned to the "owner" role instead of individual roles, which is not recommended, you can install OpenShift.

- compute.admin
- iam.securityAdmin
- iam.serviceAccountAdmin
- iam.serviceAccountUser
- iam.serviceAccountKeyAdmin
- storage.admin
- dns.admin

## Register a custom domain

One OpenShift requirement is to have a domain name. The domain name along with its subdomains for APIs and apps are used to access the OpenShift cluster console and applications including Maximo Application Suite. You can purchase a domain through Google or other domain service providers or use an existing domain name. Note that the domain name registration is done in your Google account, which is separated from your GCP account.

## Create a DNS zone in GCP

You can create a Cloud DNS zone in your GCP project. While not required, you may use a subdomain for OpenShift clusters, for example, gcp.mydomainxxx.com, leaving submains available for other uses.

![](RackMultipart20221116-1-on6sd4_html_79a2300f184810f6.png)

After a DNS zone is created, one SOA record and one NS (name server) record are added automatically. Make a note of the NS names, for example "ns-cloud-a1.googledomains.com."

## Add DNS Name Servers in Domain Registration Account

In your Google domain account, select DNS from the navigation menu and the "Default name servers" tab. Add a DNS record with the NS names you obtained from your GCP DNS zone. Save the record.

![](RackMultipart20221116-1-on6sd4_html_150a63e38c041c2d.png)

# Install Red Hat OpenShift

There are various ways of installing Red Hat OpenShift. We tested two of them on GCP: OpenShift installer and Daffy, an opensource tool managed by an IBM team. The Maximo team provides the [Ansible playbooks](https://ibm-mas.github.io/ansible-devops/playbooks/ocp/) option that you can try.

## Install OpenShift with the installer

You can download the OpenShift installer from your Red Hat account and install it. For installing OpenShift version 4.8 or higher, please refer to the document, "[Installing a cluster quickly on GCP](https://docs.openshift.com/container-platform/4.8/installing/installing_gcp/installing-gcp-default.html)".

By default, the installer creates a new cluster with 3 master nodes and 3 worker nodes. Note that the installer creates the local installation directory automatically if it does not exist already.

_./openshift-install create cluster --dir \<installation\_directory\>_

This method is also referred to "Installer Provisioned Infrastructure" or IPI. Behind the scenes the installer creates virtual machines and disks, load balancers, a virtual network including two sub networks, firewall rules, IP addresses, routes, DNS records in the selected zone, cloud NAT records, cloud routers and additional service accounts.

To customize the default configuration, you can create the "install-config.yaml" file, modify the number of worker nodes, and then create the cluster using the configuration file. Note that you may want to save a copy of the revised install-config.yaml file because it is deleted automatically when the cluster is created.

_./openshift-install create install-config --dir \<installation\_directory\>_

_./openshift-install create cluster --dir \<installation\_directory\>_

Sample install-config.yaml file

_apiVersion: v1_
_baseDomain: gcp.xxx.com_
_compute:_
_- architecture: amd64_
 _hyperthreading: Enabled_
 _name: worker_
 _platform: {}_
 _replicas: 3_
_controlPlane:_
 _architecture: amd64_
 _hyperthreading: Enabled_
 _name: master_
 _platform: {}_
 _replicas: 3_
_metadata:_
 _creationTimestamp: null_
 _name: \<replace cluster name here\>_
_networking:_
 _clusterNetwork:_
 _- cidr: 10.128.0.0/14_
 _hostPrefix: 23_
 _machineNetwork:_
 _- cidr: 10.0.0.0/16_
 _networkType: OpenShiftSDN_
 _serviceNetwork:_
 _- 172.30.0.0/16_
_platform:_
 _gcp:_
 _projectID: \<replace gcp project name here\>_
 _region: \<e.g. us-east1\>_
_publish: External_
_pullSecret: 'replace redhat pull secrets here'_
_sshKey: \<replace ssh key here\>_

When the installation is completed, you are provided with the following cluster console login info.

INFO To access the cluster as the system:admin user when using 'oc', run 'export KUBECONFIG=/home/\<username\>/Downloads/xxx/auth/kubeconfig'
INFO Access the OpenShift web-console here: [https://console-openshift-console.apps.\<clustername\>.gcp.mydomainxxx.com
](https://console-openshift-console.apps.bx48.gcp.xueopenlabs.com/)INFO Login to the console with user: "kubeadmin", and password: "nVwnr-ma84g-kUob3-766B8"

To install OpenShift in an existing environment on GCP, which is referred to as "User provisioned infrastructure" or UPI, please refer to "[Installing a cluster on GCP into an existing VPC](https://docs.openshift.com/container-platform/4.8/installing/installing_gcp/installing-gcp-vpc.html)".

## Install OpenShift with Daffy

Another easy way to install OpenShift is to use an opensource tool named Daffy, which stands for "Deployment Automation Framework For You", and has been created and maintained by one IBM team. You can find specific instructions on how to install OpenShift on GCP at the [Daffy](https://ibm.github.io/daffy/Deploying-OCP/GCP/) website.

The Daffy tool uses values stored in an environment variable to install OpenShift and other IBM products such as Cloud Pak for Data and Cloud Pak for Business Automation. To create a cluster, download and install Daffy first on a Linux machine, either Ubuntu 20.04 or REHL 8.x. The Daffy files are stored in the /data/daffy folder, not your home directory.

The sample environment file below can be used to create a cluster with 3 master nodes and 6 worker nodes.

_DAFFY\_UNIQUE\_ID="replace with email address here"_

_DAFFY\_DEPLOYMENT\_TYPE=Enablement_

_BASE\_DOMAIN="gcp.mydomainxxx.com"_

_CLUSTER\_NAME="replace with cluster name here for example mycluster"_

_OCP\_INSTALL\_TYPE="gcp-ipi"_

_OCP\_RELEASE="4.8.49"_

_VM\_TSHIRT\_SIZE="Large"_

_GCP\_PROJECT\_ID="replace with GCP project name here"_

_GCP\_REGION="us-east1"_

_OCP\_CREATE\_OPENSHIFT\_CONTAINER\_STORAGE="true"_

_CP4D\_VERSION="4.5.3"_

_CP4D\_ENABLE\_SERVICE\_DB2\_WAREHOUSE=true_

To create an OpenShift, run the command line in the "/data/daffy/env" folder. The installation takes approximately 45 minutes. Note that the ocp folder is used.

/data/daffy/ocp/build.sh \<replace with cluster name here without "-env.sh"\>

During installation, Daffy checks if all required APIs have been enabled in your GCP project. Also, Daffy checks if required roles have been assigned to the service account.

While not recommended, you could disable the role assignment validation by commenting out the _gcpValidateRolesRequired_ line in the "/data/daffy/ocp/functions.sh" file if your service account has been assigned to the "owner" role instead of individual roles.

_gcpInstallGCloud_

_gcpValidateAPIServicesRequired_

_#gcpValidateRolesRequired_

_gcpValidateConstraintsRequired_

_gcpValidateDNSZone_

When installation is completed, which takes about 35 minutes or longer, you are provided with the cluster console login info. Note that Daffy creates an additional user account, "ocpadmin".

_Here is the login info you can use for all services and_ _ **console** __:_
_#####################################################################################_
_Super User : kubeadmin_
_Password : xxx_
_Admin User : ocpadmin_
_Password : xx_
_OpenShift Web Console :_ [_https://console-openshift-console.apps.clustername.gcp.mydomainxxx.com_
](https://console-openshift-console.apps.mas16.gcp.xueopenlabs.com/)_OC Commandline : export KUBECONFIG=/var/ibm-ocp/mas16/kubeconfig_
_OC Login command : oc login_ [_https://api.\<clustername\>.gcp.mydomainxxx.com:6443_](https://api.mas16.gcp.xueopenlabs.com:6443/) _-u ocpadmin -p xxx –insecure-skip-tls-verify_
_OC Client Download :_ [_https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.8.49_
](https://mirror.openshift.com/pub/openshift-v4/clients/ocp/4.8.49)_Install Temp Files : /data/daffy/tmp/mas16/ocp_
_openshift-install Dir : /data/daffy/tmp/mas16/ocp/ocp-install_

# Use NFS Server backed Storage for OpenShift

Red Hat has re-branded OpenShift Container Storage (OCS) as Red Hat Open Data Foundation (ODF) since OpenShift 4.9 was released in 2021. While use of ODF, currently in technical preview on GCP, is the storage path going forward, a network file server (NFS) is a viable option for applications requiring support for "Read Write Many" (RWX) storage mode, for example, attached documents in Maximo. One benefit with NFS backed storage is that applications and APIs off the OpenShift cluster can access the shared storage when adequate permissions are granted. Refer to Red Hat [OpenShift Data Foundation](https://www.redhat.com/en/technologies/cloud-computing/openshift-data-foundation) for more details.

## Create a Filestore Instance

You can create an NFS server in your GCP project from the cloud console or using the gcloud command line. From the cloud console, search for "Filestore" and then create an instance.

![](RackMultipart20221116-1-on6sd4_html_a5d6b565094fcdd9.png)

Select the same VPC network for the cluster. Choose the instance type (Basic, Enterprise, High Scale), storage type (HDD, SSD), and storage capacity (ranging from 1TB to 64 TiB). For region and zone, select ones that are in the same region and zone or close to your cluster. Enter the file share name, and make a note it, along with the IP address for the instance.

When the instance is created, a VPC network peering is automatically created, which enables Filestore to access the VPC. On the other hand, any Compute Engine VM or GKE cluster can access any Filestore instance that's on the same VPC network. With the connectivity to the Filestore the storage pods in OpenShift can read and write data it.

![](RackMultipart20221116-1-on6sd4_html_b7c3436f021a5751.png)

Filestore provides data encryption at rest and in transit, and comes with two data recovery options, backups and snapshots. For high availability, use Enterprise tier Filestore instances, which are regional resources. Refer to GCP's [Filestore documentation](https://cloud.google.com/filestore/docs/overview) for more details.

##

## Deploy a Dynamic Provisioning Storage Class

Google Filestore is a fully managed storage service and more scalable than one single NFS server. When the Filestore instance is created and file share enabled, the NFS server is ready without any configuration. You can use the IP address and file share name to create OpenShift persistent volume claims (PVC) and persistent volumes (PV) directly.

However, to create storages dynamically, it's necessary to deploy a storage provision provider. For that we use the opensource project, "[Kubernetes NFS Subdir External Provisioner](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner)." NFS subdir external provisioner is an automatic provisioner that use your existing and already configured NFS server to support dynamic provisioning of PVs via PVCs. It is not recommended to use an [NFS storage provisioner](https://www.ibm.com/docs/en/mas-cd/continuous-delivery?topic=provisioner-setting-up-nfs-storage) for workloads with heavy I/O performance requirements.

To deploy the storage provider, take the following steps, as outlined on the [support](https://www.ibm.com/support/pages/how-do-i-create-storage-class-nfs-dynamic-storage-provisioning-openshift-environment) web page:

1. Copy or download the yaml files from the /deploy folder in the github repo – class.yaml, deployment.yaml, rbac.yaml, test-claim.yaml and test-pod.yaml. Note that the namespace "default" is used in the files, but you can change it.

1. Replace the namespace in the deployment.yaml file. Also, replace the value of PROVISIONER\_NAME in the spec section and change it from "k8s-sigs.io/nfs-subdir-external-provisioner" to something else, for example, "nfs-storage-provisioner". Also, replace the IP address and file share path in the env section and the volumes section.

1. Replace the namespace in the rbac.yaml file. The namespace appears in a few places so do a global search and replace.

1. Log in to the cluster and run the command lines to create a namespace, for example "nfs", storage class deployment, the storage class, and role based access (rbac).

_Oc create namespace nfs_

_oc create -f deployment.yaml_

_oc create -f class.yaml_

_oc create -f rbac._yaml

1. Create a role based on "hostmount-anyuid" which allows to provision the storage and assign the role to the service account, for example, "nfs-client-provisioner", which is specified in the rbac.yaml file. More info on the role at "[Managing security context constraints](https://docs.openshift.com/container-platform/4.11/authentication/managing-security-context-constraints.html)."

_Oc create role use-scc-hostmount-anyuid –verb=use --resource=scc –resource-name=hostmount-anyuid -n nfs_

_ca dm policy add-role-to-user use-scc-hostmount-anyuid -z nfs-client-provisioner –role-namespace nfs -n nfs_

You can check the OpenShift cluster to make sure that the deployment is successful. You can scale the deployment from the default 1 pod to 2 or more.

![](RackMultipart20221116-1-on6sd4_html_1c0e788981a6eee7.png)

You can create a PVC and a pod to test the storage class using test-claim.yaml and test-pod.yaml files, or a sample yaml file that includes PVC and pod in one file.

_apiVersion: v1_

_kind: PersistentVolumeClaim_

_metadata:_

_name: test-nfs-provisioner_

_namespace: nfs_

_spec:_

_storageClassName: nfs-client_

_accessModes:_

_- ReadWriteMany_

_resources:_

_requests:_

_storage: 10Mi_

_apiVersion: v1_

_kind: Pod_

_metadata:_

_labels:_

_run: ubuntu_

_name: ubuntu-test-nfs_

_namespace: nfs_

_spec:_

_containers:_

_- image: ubuntu_

_name: ubuntu_

_resources: {}_

_command: ["sleep", "3600"]_

_volumeMounts:_

_- mountPath: /nfs_

_name: nfs-vol_

_volumes:_

_- name: nfs-vol_

_persistentVolumeClaim:_

_claimName: test-nfs-provisioner_

# The Infrastructure of OpenShift Deployment on GCP

With the installer and Daffy deployment options, the OpenShift cluster is deployed in one region, and the master nodes and worker nodes of the cluster are deployed in different zones in the same region on Google Cloud.

## Network Topology

You can view a network topology of the deployment in your GCP project, as shown below. The external load balancing is used to optimize network traffic from different regions, whereas the internal load balancing is used for internal network traffic within the OpenShift cluster.

When a Filestore instance is created, a network peering for the instance is created automatically, enabling access between the instance and pods in the cluster.

![](RackMultipart20221116-1-on6sd4_html_8c568365a41b84b2.png)

##

## Cloud Services in OpenShift Deployment

Depending on the size of the OpenShift cluster, virtual instances of Compute Engine for master nodes and worker nodes along with persistent disks are created. A cloud DNS is required prior to the deployment. After the deployment, several GCP cloud services are created and utilized, including one VPC network with two subnets, external and internal load balancers, cloud router, internal and external IP addresses, firewall rules, cloud routes and cloud NAT.

You can find other deployment options on GCP in [Red Hat documentation](https://docs.openshift.com/container-platform/4.11/installing/installing_gcp/installing-restricted-networks-gcp.html), for example, installing OpenShift into an existing VPC, installing a private cluster, installing a cluster using Deployment Manager templates, or installing a cluster using user provisioned infrastructure (UPI).

## Infrastructure Architecture

The diagram below illustrates the high-level infrastructure of the OpenShift deployment on GCP.

![](RackMultipart20221116-1-on6sd4_html_d9581bdc05503c87.png)

#

# Install Maximo Application Suite

IBM Maximo Application Suite (MAS) enables you to monitor, manage and maintain your

assets in a single platform and streamline your operations. With latest releases 8.x, you can deploy MAS in the cloud.

Installing MAS takes two steps: install MAS Core first and then activate individual applications. To install MAS core, you run an Ansible playbook, which automates the installation process. Please refer to the document "[OneClick Install for MAS Core](https://ibm-mas.github.io/ansible-devops/playbooks/oneclick-core/)" for more details.

## Define Environment Variables

It's worth noting that installing MAS Core requires IBM entitlement key and MAS licensing information and define several environment variables. Note that we use the NFS storage class.

_export MAS\_INSTANCE\_ID=inst1_

_export INSTANCE=inst1_

_#cataglog source: ibm-opertor - catalog / release -mas-curated-cat_

_export COMMON\_SERVICES\_CATALOG\_SOURCE=release-mas-curated-cat_

_export SLS\_CATALOG\_SOURCE=release-mas-curated-cat_

_export SLS\_LICENSE\_ID=xxx_

_export SLS\_LICENSE\_FILE=~/masconfig/entitlement.lic_

_export PROJECT\_CPFS\_OPS=ibm-common-services_

_export PROJECT\_CPD\_OPS=cpd-operators_

_export PROJECT\_CATSRC=openshift-marketplace_

_export PROJECT\_CPD\_INSTANCE=cpd-instance_

_export PROJECT\_TETHERED=cpd-tethered_

_export IBM\_ENTITLEMENT\_KEY=xxx_

_export IBM\_ENTITLEMENT\_USER=cp_

_export IBM\_ENTITLEMENT\_SERVER=cp.icr.io_

_#export ARTIFACTORY\_USERNAME=xxx_

_#export ARTIFACTORY\_APIKEY=xxx_

_export MAS\_ICR\_CP=cp.icr.io/cp_

_export MAS\_ICR\_CPOPEN=icr.io/cpopen_

_export MAS\_ENTITLEMENT\_USERNAME=cp_

_export MAS\_CONFIG\_DIR=~/masconfig_

_export MAS\_CATALOG\_SOURCE=release-mas-curated-cat_

_export MAS\_CHANNEL=8.x_

_export UDS\_CONTACT\_EMAIL=xxx@xxx.com_

_export UDS\_CONTACT\_FIRSTNAME=xxx_

_export UDS\_CONTACT\_LASTNAME=xxx_

_export PROMETHEUS\_STORAGE\_CLASS=nfs-client_

_export PROMETHEUS\_ALERTMGR\_STORAGE\_CLASS=nfs-client_

_export PROMETHEUS\_USERWORKLOAD\_STORAGE\_CLASS=nfs-client_

_export GRAFANA\_INSTANCE\_STORAGE\_CLASS=nfs-client_

_export MONGODB\_STORAGE\_CLASS=nfs-client_

_export DB2\_DATA\_STORAGE\_CLASS=nfs-client_

_export KAFKA\_STORAGE\_CLASS=nfs-client_

_export UDS\_STORAGE\_CLASS=nfs-client_

You can define and store the environment variables in a file, for example "mas\_var.sh", and then run the command line "_source mas\_var.sh_" to export them in the current terminal session.

## Install MAS Core

Login to the OpenShift cluster and install the MAS catalog by running the command line:

_oc apply -f release-mas-curated-cat.yaml_

The _release-mas-curated-cat.yaml_ file includes the image file in IBM Container Registry.

_apiVersion: operators.coreos.com/v1alpha1_

_kind: CatalogSource_

_metadata:_

_creationTimestamp: '2022-08-24T18:25:59Z'_

_generation: 1_

_name: release-mas-curated-cat_

_namespace: openshift-marketplace_

_resourceVersion: '1255203'_

_uid: b3f9214e-2e5c-4e8b-ab53-68052d0a5099_

_spec:_

_displayName: release-mas-curated-cat_

_image: \>-_

_cp.icr.io/cpopen/ibm-operator-catalog@sha256:d1ccb02f6d6a74736fa59714c266069c78f3fb366353cdbcaee635a5720e6f27_

_publisher: ''_

_sourceType: grpc_

_status:_

_connectionState:_

_address: 'release-mas-curated-cat.openshift-marketplace.svc:50051'_

_lastConnect: '2022-08-25T14:14:52Z'_

_lastObservedState: READY_

_registryService:_

_createdAt: '2022-08-24T18:26:18Z'_

_port: '50051'_

_protocol: grpc_

_serviceName: release-mas-curated-cat_

_serviceNamespace: openshift-marketplace_

Then, run the command line to execute the Ansible playbook. Download [Ansible](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html) and install it on your Bastion machine if you have not done so.

_ansible-playbook ibm.mas\_devops.oneclick\_core_

When the installation is completed successfully, you are provided with the following login information.

_TASK [ibm.mas\_devops.suite\_verify : gencfg-wsl : Watson Studio Installation Summary:] \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*_

_ok: [localhost] =\> {_

_"msg": [_

_"Maximo Application Suite is Ready, use the superuser credentials to authenticate",_

_"Admin Dashboard ... https://admin.inst1.apps.\<clustername\>.gcp.mydomainxxx.com",_

_"Username .......... eXTbAASkwXK4gJxfHoeWQi1HSu0pADPr",_

_"Password .......... cwtq9z5ApGhZF8K1hROcsLXkaL7zcPQE" ]}_

# Activate MAS Manage

MAS Manage is usually the first application you activate, which involves three key steps:

- Deploy IBM Cloud Pak for Data
- Create a DB2 Warehouse instance and configure database connection
- Activate Manage and other applications

Alternatively, you can run an Ansible Playbook to create a DB2 Warehouse, configure database connection and activate MAS Manage all in one step. Please refer to the document, "[Install Manage Application](https://ibm-mas.github.io/ansible-devops/playbooks/oneclick-manage/)".

## Deploy IBM Cloud Pak for Data

Maximo Application Suite (MAS) requires database to store data using IBM DB2, DB2 Warehouse, or other types of databases. To use IBM DB2 or DB2 Warehouse, you can install DB2 or DB2 Warehouse directly, or deploy IBM Cloud Pak for Data (CP4D) which comes with DB2 and DB2 Warehouse.

To install CP4D version 4.5.x or higher, you can refer to the document, "[Installing Cloud Pak for Data](https://www.ibm.com/docs/en/cloud-paks/cp-data/4.5.x?topic=installing)." However, it is highly recommended that you use Daffy. Add the two environment variables to the same xxx-env.sh file.

_CP4D\_VERSION="4.5.3"_

_CP4D\_ENABLE\_SERVICE\_DB2\_WAREHOUSE=true_

Run the command line in the /data/daffy/env folder. Note that the cp4d folder is used.

_/data/daffy/cp4d/build.sh \<replace with cluster name here without "-env.sh"\>_

During the build process, Daffy runs the command to apply two storage classes.

_/data/daffy/tmp/cpdcli/cpd-cli-linux-EE-11.0.0-20/cpd-cli manage apply-cr --components=cpfs,scheduler,cpd\_platform --release=4.5.0 --cpd\_instance\_ns=cpd-instance --block\_storage\_class=ocs-storagecluster-ceph-rbd --file\_storage\_class=ocs-storagecluster-cephfs --license\_acceptance=true_

When the installation is completed, which takes approximately 1 hour and 45 minutes or longer, you are provided with the following CP4D control plane login information.

_Here is the login info for the CP4D Navigator console_

_################################################################_

_Super User : admin_

_Password : rvGcd27nHWRd_

_CP4D Web Console :_ _   __https://cpd-cpd-instance.apps.\<clustername\>.gcp.mydomainxxx.com_

##

## Create DB2 Warehouse Instance

First, log in to the CP4D control plane and create a DB2 Warehouse instance.

![](RackMultipart20221116-1-on6sd4_html_2bc58ed7ce8e2ab7.png)

Follow the steps of the database instance creation from the portal:

- Type: Select the DB2 Warehouse version installed.
- Configure: For storage structure, select "Separate locations for all data". For production, select "Deploy database on dedicated nodes".
- Advanced configuration: For workload, select "Operational Analytics".
- System storage: Select the storage class, for example, "nfs-client". Choose the data size.
- User storage: Select the storage class, for example, "nfs-client". Choose the data size. For access mode, select "ReadWriteOnce" or "ReadWriteMany".
- Backup storage: Select the storage class, for example, "nfs-client". Choose the data size.
- Transaction logs storage: Select the storage class, for example, "nfs-client". Choose the data size.

Create the database instance. Make a note of the host name and the JDBC Connection URL (SSL).

_jdbc:db2://\<CLUSTER\_ACCESSIBLE\_IP\>:32319/BLUDB:user=-;password=\<password\>;securityMechanism=9;sslConnection=true;encryptionAlgorithm=2_

![](RackMultipart20221116-1-on6sd4_html_cf89f7522ba5691f.png)

Click the "Download SSL Certificate" button to download the certificate. You will need the info when configuring the database connection for MAS.

Create a new user, for example "maximo" and assign both the "administrator" and "user" roles to it.

## Create Database Schema for Manage

Configure DB2 Warehouse and create MAS schema by running a set of scripts. Please refer to the document, "[Configuring Db2 Warehouse](https://www.ibm.com/docs/en/maximo-manage/continuous-delivery?topic=deployment-configuring-db2-warehouse)".

## Configure Database Connection

Next, log into the MAS console and select the Administration page (the gear icon) at upper right corner. Select Configuration from the navigation menu on the left side of the page. You can configure database connection at the System level, Workspace level and other levels or scopes. Please refer to the document, "[Configuration Settings](https://www.ibm.com/docs/en/mas85/8.5.0?topic=administering-configuring-suite)".

To configure database connection for the workspace, select the workspace name, for example "masdev". Copy and paste the connection string you noted from the previous step. Remove the user and password from it. Replace CLUSTER\_ACCESSIBLE\_IP with the host name from the previous step.

_jdbc:db2://\<hostname\>:32319/BLUDB:securityMechanism=9;sslConnection=true;encryptionAlgorithm=2;_

Enter the user credentials you created at the previous step. Open the certificate you downloaded, copy and paste the text to the Certificates box and confirm/save it. Save the connection string. It takes a few minutes to update the connection string in MAS.

![](RackMultipart20221116-1-on6sd4_html_9eef06cbd1faa2d8.png)

## Activate MAS Manage

From the MAS console, select Workspace, activate "Manage" and deploy "Optimizer". This process can take some time.

![](RackMultipart20221116-1-on6sd4_html_65d421316cad2637.png)

When the activation is completed, the workspace and Manage show "Ready".

Next, create a new user or reset password for an existing user such as "maxadmin". Then log out the admin account.

## Launch MAS Manage

You can find the web URL for the Manage application using the oc command or from the OpenShift cluster console.

- Run the oc command: _oc get routes -n \<install instance id\>-manage_

- From the cluster console, navigate to Networking, select the namespace \<install instance id\>-manage, and click on Routes.

To access the Manage application, add /maximo to the end of the route, or replace "manage" with "home" in the URL without adding "/maximo".

_https://masdev.manage.\<install instance id\>.apps.\<cluster name\>.gcp.mydomainxxx.com/__ **maximo** _

_https://masdev. __**home**__.\<install instance id\>.apps.\<cluster name\>.gcp.mydomainxxx.com_

Log in with a user account using the revised URL. Click on the Applications icon (9 dots) at upper right corner. Select Manage to see the dashboard.

![](RackMultipart20221116-1-on6sd4_html_d7a2054d0fe660d3.png)

#

# Storages for OpenShift and Maximo

OpenShift Data Foundation (ODF) is in technical preview on GCP and therefore NFS backed storage is used to provide the RWX storage mode for Maximo. In the test environment, the custom storage class, "nfs-client", is used for Maximo only, whereas the built-in storage classes are used for OpenShift.

## OpenShift Storage Classes

With the installer option, three storage classes are available for the cluster: standard, standard-csi, and pd-ssd. They support "read write once" mode (RWO).

![](RackMultipart20221116-1-on6sd4_html_572f68dca07d67cc.png)

Daffy adds three storage classes, ocs-storagecluster-cephfs, ocs-storagecluster-ceph-rbd and openshift-storage.noobaa.io when the environemnt variable, _OCP\_CREATE\_OPENSHIFT\_CONTAINER\_STORAGE,_ is set to "true". They support both "read write once" (RWO) and "read write many" (RWX) modes.

## Maximo DB2WH Storage

By default, OpenShift (version 4.8) uses a few storage classes in various namespaces. The table below summarizes the usage of storage classes from the console page of the OpenShift PVCs.

| **Storage Class** | **Namespace** |
| --- | --- |
| standard-csi | ibm-common-servicesgrafanamongoceopenshift-monitoringopenshift-user-workload-monitoring |
| pd-ssd | openshift-storage |
| ocs-storagecluster-cephfs | cpd-instance (CP4D) |
| ocs-storagecluster-ceph-rbd | openshift-storagecpd-instance (CP4D) |
| nfs-client | cpd-instance (CP4D) |

For Maximo, DB2WH data, backup, temps, active logs and meta are in storages provisioned by the "nfs-client" storage class.

![](RackMultipart20221116-1-on6sd4_html_4ba4b0b1419f8f54.png)

# Troubleshoot MAS Manage Activation

## Check the database connection string in Pods

Select project \<install instance id\>-manage from the cluster console. Search Pods with the name "maxinst". Open the pod and a Terminal session. Run the command line in the directory, "/opt/IBM/SMP/maximo/tools/maximo/internal".

_./querycount.sh -qcount -tdummy\_table_

In the example, we find that a ";" instead of ":" was used in the connection string. Fix the connection string in the configuration section on the MAS administration page.

_jdbc:db2://\<hostname\>:32319/BLUDB __**;**__ securityMechanism=9;sslConnection=true;encryptionAlgorithm=2;_

![](RackMultipart20221116-1-on6sd4_html_21155d0b801e7b50.png)

## Check "ManageWorkspace" custom resource

Go to CustomResourceDefinitions under the Administration section on the cluster console. Search "workspace". Select "ManageWorksapce". Go to the Instances tab and select the instance named "\<install instance id\>-\<workspace name\>". Check if the binding is set at workspace or workspace-application and update it if necessary.

 ![](RackMultipart20221116-1-on6sd4_html_ef72356ed2d97c66.png)

## Check "jdbc" custom resource

Go to CustomResourceDefinitions under the Administration section on the cluster console. Search "jdbc".

There are likely two resources or more in the \<install instance id\>-core namespace, one for system and one for workspace. Open the System resource and check the YAML detail.

![](RackMultipart20221116-1-on6sd4_html_59fc55eeeeb9d39e.png)

# Shut Down the OpenShift Cluster

For dev/test environment, you may want to shut down the OpenShift environment to reduce costs. You can run the command line below to shut down the cluster gracefully.

_for node in $(oc get nodes -o jsonpath='{.items[\*].metadata.name}'); do oc debug node/${node} -- chroot /host shutdown -h 1; done _

To resume the Cluster, you can restart each master node instance first and then each worker node instance from the cloud console. Wait until all virtual instances are up and running before running any commands against the cluster.

Do not shut down the virtual machine instances directly from GCP cloud console. If you do so, the cluster may not work properly when you resume the virtual machine instances.

# Delete OpenShift Cluster and MAS Deployment

You can delete the project in GCP if you want to delete OpenShift or MAS. To delete all resources without deleting the project, you can do so in the GCP cloud, which is straightforward, or run "gcloud" commands.

Under the VM Instances section:

- delete all virtual machine instances
- delete all disks
- delete 7 service accounts

Under the Hybrid Connectivity section:

- delete 1 cloud routers

Under the Network Services section:

- delete 3 loadbalancers including healthchecks
- delete 1 dns zone including 3 recordsets
- delete 2 nats

Under the VPC networks section:

- delete 1 vpc network peering (if filestore is configured)
- delete 10+1 firewall rules (1 for ssh to nfs vm)
- delete 2 IP addresses
- delete 1 routes
- delete 2 subnets
- delete vpc

# Next Steps

In the document we provide basic instructions on how to install OpenShift, storage, database and Maximo Application Suite (MAS) on Google Cloud and some troubleshooting tips. Please refer to other documents on advanced topics such as how to install OpenShift in existing environments or private clusters, how to extend OpenShift with additional nodes and how to deploy MAS based on a reference architecture.
